---
title: "Project 3: The Ames Housing Dataset"
author: "Sean Tsai, Olli Rissanen, Kit Tsang, Ying Jie Chin, Abhinav Alluri"
date: "4/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Description of the Data and our Project

The Ames Housing Dataset is an incredible dataset with 79 explanatory variables describing almost every aspect of residential homes in Ames, Iowa, a city that serves as a home for the Iowa State University and has a population of around 60,000. 

The 79 variables focus on the quality and quantity of many physical attributes of the properties, and most of the variables are exactly the type of information that a typical home buyer wants to know about a potential property. There are variables about various area dimensions (such as lot size and different square footage variables), discrete variables that describe the number of items (e.g. bathrooms) occurring within the house, and categorical variables classifying e.g. streets and neighborhoods. 

With the dataset and based on the listed variables, we aim to predict sale prices of different Ames, Iowa properties using different machine learning models. For our predictions, we plan to use ANN, kNN, SVR, and Random Forest models, as well as a prediction model that combines all these four models. 

Besides submitting this file as our Project 3 deliverable, we will make our final predictions on a special testing dataset provided in Kaggle and use those predictions to participate in a Kaggle Competition that is based on the Ames Housing Dataset.



# General Data Exploration


## Importing Data

```{r}
# Importing the train part of the dataset
rawHouseData_Train <- read.csv("train.csv", stringsAsFactors = FALSE)

# Importing the test part of the dataset (used only for the final predictions for the Kaggle competition)
rawHouseData_Test <- read.csv("test.csv", stringsAsFactors = FALSE)
```


## Exploring Data

```{r}
str(rawHouseData_Train)
summary(rawHouseData_Train)
```



# Data Cleaning


## Converting Data into Characters for Data Cleaning

```{r}
# We need to use characters for our Boruta Screening of features
CharCategory <- c(names(Filter(is.character, rawHouseData_Train)), "MSSubClass")
```


## Identifying NAs with mice

```{r}
# Importing the required libraries
library(VIM)
library(mice)

# Identifying the percentage of NAs in each column
mice_plot <- aggr(rawHouseData_Train, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(rawHouseData_Train), cex.axis=.4,
                    gap=3, ylab=c("Missing data","Pattern"))
```


## Removing and Imputing NAs

```{r}
# Dropping the row with NA in Electrical column (only one missing observation for the variable)
rawHouseData_Train <- rawHouseData_Train[!is.na(rawHouseData_Train$Electrical),] 

# Dropping the GarageYrBlt column - imputing NAs with year = 0 might affect regression and other garage variables exist
rawHouseData_Train <- rawHouseData_Train[,!names(rawHouseData_Train)=="GarageYrBlt"] 

# Imputing NAs in MasVnrArea with 0
rawHouseData_Train$MasVnrArea <- ifelse(is.na(rawHouseData_Train$MasVnrArea) == TRUE, 0, rawHouseData_Train$MasVnrArea)

# Imputing NAs by "Missing" for characters for the rest of the columns (i.e. e.g. PoolQC, MiscFeature, Alley), where NA means that attribute does not exist
for(i in CharCategory){
  rawHouseData_Train[,i] = ifelse(is.na(rawHouseData_Train[,i]), "Missing", rawHouseData_Train[,i])
  rawHouseData_Test[,i] = ifelse(is.na(rawHouseData_Test[,i]), "Missing", rawHouseData_Test[,i])
}

# Imputing the NAs for Lot Frontage using predictive mean matching 
rawHouseData_Train <- mice(rawHouseData_Train, m=5, maxit = 50, method = 'pmm', seed = 100)
rawHouseData_Train <- mice::complete(rawHouseData_Train)
```


## Further Cleaning with Boruta Training

```{r}
# Boruta Train package to identify important features (https://www.datacamp.com/community/tutorials/feature-selection-R-boruta)

# Importing the required library
library(Boruta)

# Executing Boruta Training
set.seed(233)
boruta_Train <- Boruta(rawHouseData_Train[names(rawHouseData_Train) != "Id" & names(rawHouseData_Train) != "SalePrice" ], rawHouseData_Train$SalePrice ,  doTrace = 2)
final.boruta <- TentativeRoughFix(boruta_Train)
print(final.boruta)
plot(final.boruta)

# Selecting only the features that passed the Boruta Screening
cleanHouseData_Train <- rawHouseData_Train[getSelectedAttributes(final.boruta, withTentative = F)]

# Converting characters back into factors
cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)] <- lapply(cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)], as.factor)

# Adding SalePrice back into the train data
cleanHouseData_Train$SalePrice <- rawHouseData_Train$SalePrice
```


## A Look into the cleaned data

```{r}
str(cleanHouseData_Train)
summary(cleanHouseData_Train)
```



# Linear and Stepwise Regressions

In this section, we conduct Linear and Stepwise Regressions to explore variable signifigance.


## Building a Linear Regression

```{r}
SalePrice_LinReg = lm(SalePrice ~ ., data = cleanHouseData_Train)
```

### Summary of Linear Regression Performance

```{r}
summary(SalePrice_LinReg)
```


## Building a Stepwise Regression

```{r}
# Importing the required library
library(MASS)

# Creating the regression
SalePrice_StepReg = stepAIC(SalePrice_LinReg, direction = "both", trace = FALSE)

```

### Summary of the Stepwise Regression Performance

```{r}
summary(SalePrice_StepReg)
```



# Creating a Normalized Version of the Data


## Converting Categorical Data to Numeric Data

```{r}
cleanHouseData_Train[] <- data.matrix(cleanHouseData_Train)
```


## Normalization function

```{r}
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}
```


## Executing the normalization

```{r}
# Normalizing Data (excluding SalePrice column)
Norm_HouseData <- as.data.frame(lapply(cleanHouseData_Train[,!names(cleanHouseData_Train) == "SalePrice"], normalize))

# Adding the SalePrice variable to the normalized data
Norm_HouseData$SalePrice <- cleanHouseData_Train$SalePrice
```


## A look into the Normalized Data

```{r}
str(Norm_HouseData)
summary(Norm_HouseData)
```



# Dividing Data into Train and Test parts


## Randomizing Data and Creating Train and Test IDs

```{r}
set.seed(300)
trainID <- sample(1:nrow(cleanHouseData_Train),0.8*nrow(cleanHouseData_Train))
testID <- setdiff(1:nrow(cleanHouseData_Train), trainID)
```


## Dividing the Original Cleaned Data into Train and Test parts

Here we divide the original cleaned data (i.e. cleanHouseData_Train) to Train and Test parts to be used in our Random Forest and SVR Models.

```{r}
OG_HouseData_Train <- cleanHouseData_Train[trainID,]
OG_HouseData_Test <- cleanHouseData_Train[testID,]
```


## Dividing the Normalized Data into Train and Test parts

Here we divide the normalized data into Train and Test parts to be used in our ANN and kNN Models.

```{r}
Norm_HouseData_Train <- Norm_HouseData[trainID,]
Norm_HouseData_Test <- Norm_HouseData[testID,]
```



# Predictive Models


## Random Forest Model

### Importing the required libraries

```{r}
library(caret)
library(randomForest)
```


### Renaming Train and Test Data for Clarity

```{r}
RF_HouseData_Train <- OG_HouseData_Train
RF_HouseData_Test <- OG_HouseData_Test
```

### Creating the Random Forest Model

To decide the variable combination used in the Random Forest Model, we tested 2 different variable combinations. The first combination of variables included all variables in RF_HouseData_Train, which includes all the variables deemed as important by Boruta. The second combination of variables included all the significant variables with a p-value lower than 0.05 from the final stepwise regression model. We used the significant variables from the final stepwise regression model instead of the significant variables from our first linear regression model because the stepwise regression model was more optimimized with a lower AIC.

Using only the significant variables from the stepwise regression produced the better accuracy out of the two options (measured by the strength of correlation between the predicted house prices and the actual house prices listed in the test part of the data) and, thus that combination of variables was chosen for our Random Forest Model. 

```{r}
set.seed(300)
RF_Model <- randomForest(SalePrice ~ MSZoning + LotFrontage + LotArea + LotShape + 
    LandContour + LandSlope + Neighborhood + Condition1 + BldgType + 
    HouseStyle + OverallQual + OverallCond + YearBuilt + Exterior1st + 
    MasVnrArea + ExterQual + BsmtQual + BsmtFinType1 + X2ndFlrSF + 
    GrLivArea + BsmtFullBath + FullBath + BedroomAbvGr + KitchenAbvGr + 
    KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + GarageCars + 
    GarageQual + GarageCond + WoodDeckSF + SaleCondition, data = RF_HouseData_Train)

# Basic information about the model
RF_Model

# Plotting the Random Forest Model
plot(RF_Model)
```

### Random Forest Prediction and Results

```{r}
RF_Prediction <- predict(RF_Model, RF_HouseData_Test, type = "response")
cor(RF_Prediction,RF_HouseData_Test$SalePrice)
```

There is a strong correlation of `r cor(RF_Prediction,RF_HouseData_Test$SalePrice), digits = 3)` between the house prices predicted by the Random Forest Model and actual prices. This implies a good prediction and that the predicted prices follow a similar trend with the real prices. Compared to the correlation achieved when when all the variables from the Boruta screening were used, the achieved correlation with this set of variables is higher by approximately 0.0006.


## ANN Model

### Importing the required libraries

```{r}
library(nnet)
require(RCurl)
```

### Renaming Train and Test Data for Clarity

```{r}
ANN_HouseData_Train <- Norm_HouseData_Train
ANN_HouseData_Test <- Norm_HouseData_Test
```

### Creating the ANN Model

In creating the ANN Model, we experimented with the same 2 variable combinations as described above for the Random Forest Model. Again, the best accuracy was achieved using only the significant variables from the stepwise regression. Thus, our final ANN Model uses that combination of variables.

```{r}
set.seed(300)
ANN_Model <- nnet(SalePrice ~ Neighborhood + HouseStyle + MasVnrArea + FullBath + MSZoning + Exterior1st + BsmtFinType1 + X2ndFlrSF + BedroomAbvGr + TotRmsAbvGrd + Functional + LotFrontage + LandContour + LandSlope + Condition1 + YearBuilt + ExterQual + KitchenAbvGr + KitchenQual + Fireplaces + WoodDeckSF + SaleCondition + LotArea + LotShape + BldgType + OverallQual + OverallCond + BsmtQual + GrLivArea + BsmtFullBath + KitchenQual + + GarageCars + GarageQual + GarageCond,
    data=ANN_HouseData_Train,
    size=10, linout=TRUE, skip=TRUE, MaxNWts=10000, trace=FALSE, maxit=100)

```

### ANN Prediction and Results

```{r}
# ANN Prediction
ANN_Prediction <- predict(ANN_Model, newdata=ANN_HouseData_Test[,!names(ANN_HouseData_Test)=="SalePrice"])

# ANN Results
cor(ANN_Prediction, ANN_HouseData_Test$SalePrice)
```

The ANN Model predictions and actual house prices have a correlation of `r round(cor(ANN_Prediction, ANN_HouseData_Test$SalePrice), digits = 3)`. This correlation is approximately 0.037 higher than the correlation obtained when all the variables from Boruta screening were used.


## Support Vector Regression Model

### Importing the required library

```{r}
library(kernlab)
```

### Renaming Train and Test Data for Clarity

```{r}
SVR_HouseData_Train <- OG_HouseData_Train
SVR_HouseData_Test <- OG_HouseData_Test
```

### Creating the SVR Model

For the SVR Model, we first used all the variables from Boruta screening to construct the SVR model and optimise for the best kernel. Next, using the kernel with the highest accuracy, we tested the 2 variable combinations as mentioned above. 

The model shown below is the model which has the highest accuracy or correlation strength with the test data, using variables from Boruta screening and the rbfdot kernel. We have previously tested other kernels including vanilladot, tanhdot and polydot. However, all of these had lower correlation strength with the test data, with correlation at 0.8414073, -0.02670659 and 0.8414184 respectively. 

```{r}
# Constructing SVR Model with variables from Boruta screening
set.seed(300)
SVR_Model <- ksvm(SalePrice ~ ., data = SVR_HouseData_Train,
                          kernel = "rbfdot")
```

### SVR Model Prediction and Results

```{r}
# SVR Model Prediction
SVR_Prediction <- predict(SVR_Model, SVR_HouseData_Test)

# SVR Model Results
cor(SVR_Prediction, SVR_HouseData_Test$SalePrice)
```

With the Boruta screening variables, the SVR Model prediction (using the rbfdot kernel) and actual house prices have a correlation of `r round(cor(SVR_Prediction, SVR_HouseData_Test$SalePrice), digits = 3)`, which is much higher than the other 3 kernels. 

Having optimised our model for the kernel, we now go on to find out if the 2nd variable combination using only significant variables from the stepwise regression would have a higher accuracy.

### Creating SVR Model 2 using significant variables from Stepwise Linear Regression

```{r}
# Constructing SVR Model 2 with significant variables from stepwise linear regression
set.seed(300)
SVR_Model2 <- ksvm(SalePrice ~ Neighborhood + HouseStyle + MasVnrArea + FullBath + MSZoning + Exterior1st + BsmtFinType1 + X2ndFlrSF + BedroomAbvGr + TotRmsAbvGrd + Functional + LotFrontage + LandContour + LandSlope + Condition1 + YearBuilt + ExterQual + KitchenAbvGr + KitchenQual + Fireplaces + WoodDeckSF + SaleCondition + LotArea + LotShape + BldgType + OverallQual + OverallCond + BsmtQual + GrLivArea + BsmtFullBath + KitchenQual + + GarageCars + GarageQual + GarageCond, data = SVR_HouseData_Train,
                          kernel = "rbfdot")
```

### SVR Model 2 Prediction and Results

```{r}
# SVR Model Prediction
SVR_Prediction2 <- predict(SVR_Model2, SVR_HouseData_Test)

# SVR Model Results
cor(SVR_Prediction2, SVR_HouseData_Test$SalePrice)
```

From the results above, it was found that the SVR model which used Boruta screening performed better, with a higher correlation of `r round(cor(SVR_Prediction, SVR_HouseData_Test$SalePrice), digits = 3)` compared to the SVR model with a correlation of `r round(cor(SVR_Prediction, SVR_HouseData_Test$SalePrice), digits = 3)`. Thus, for SVR, we will be choosing the first SVR model with all the Boruta variables as our final SVR model.



## KNN Model

### Renaming Train and Test Data for Clarity

```{r}
kNN_HouseData_Train <- Norm_HouseData_Train
kNN_HouseData_Test <- Norm_HouseData_Test

```

### Creating the kNN Model

For kNN Model, we once again tested the same potential variable combinations. Using xxxxxx seemed to produce the best accuracy and, thus was chosen for our final kNN Model.


```{r}
# Training Control using a repeated 10-fold cross-validation to find the optimal k-value
trControl <- trainControl(method = 'repeatedcv',number = 10, repeats = 3)
```

```{r}
# Creating the model
set.seed(300)
kNN_Model <- train(SalePrice ~ ., data = kNN_HouseData_Train, tuneGrid = expand.grid(k=1:70), method = 'knn', trControl = trControl, preProc = c('center', 'scale'))

# Plotting the kNN model
plot(kNN_Model)

# Looking into the variable importance within the kNN Model
varImp(kNN_Model)
```

The varImp function helps us see that OverallQual, GrLivArea, and TotalBsmtSF are some of the most meaningful variables in predicting the sale price of a house. Meanwhile, variables like the number of fireplaces, or the year the garage was built are not as significant in this prediction. 

### kNN Model Prediction and Results

```{r}
kNN_Prediction <- predict(kNN_Model, newdata = kNN_HouseData_Test)
plot(kNN_Prediction ~ kNN_HouseData_Test$SalePrice)
cor(kNN_Prediction, kNN_HouseData_Test$SalePrice)
```

The plot above shows the actual sale price of a house against the predicted sale price of the house. Since the plot is pretty linear - with the exception of a few outliers - and has a slope close to one, the kNN Model seems to be a quite decent model in predicting the sale price of a house. 

For kNN, the model which included all the Boruta screening variables had a 0.0036874 higher accuracy compared to the model with only the significant variables from Stepwise Linear Regression, Thus, our final model would be the kNN model with all the Boruta screening variables

For our final model, the kNN Model predictions and actual house prices have a correlation of `r round(cor(kNN_Prediction, kNN_HouseData_Test$SalePrice), digits = 3)` which supports the view of the kNN Model as a decent predictor for house prices seen already from the plot. 


# Combined Prediction Model


## Building a dataframe containing predictions of all models

```{r}
House_Prices <- data.frame(RF_Prediction,ANN_Prediction,SVR_Prediction,kNN_Prediction,OG_HouseData_Test$SalePrice)
colnames(House_Prices) <- c("Random Forest", "ANN", "SVR", "kNN", "True Price")
```


## Combined Model Prediction

We build the prediction for our combined prediction model by taking an average of the predictions of the individual models, weighted by the relative correlation strength of each model.

```{r}
# Correlations
RF_Cor <- cor(House_Prices$`Random Forest`,House_Prices$`True Price`)
ANN_Cor <- cor(House_Prices$ANN,House_Prices$`True Price`)
SVR_Cor <- cor(House_Prices$SVR,House_Prices$`True Price`)
kNN_Cor <- cor(House_Prices$kNN,House_Prices$`True Price`)

# Sum of Correlations
SUM_Of_Cor <- RF_Cor + ANN_Cor + SVR_Cor + kNN_Cor

# Weights
RF_Weight <- RF_Cor / SUM_Of_Cor
ANN_Weight <- ANN_Cor / SUM_Of_Cor
SVR_Weight <- SVR_Cor / SUM_Of_Cor
kNN_Weight <- kNN_Cor / SUM_Of_Cor

# Taking the weighted average of predictions
House_Prices[,"Weighted Average"] <- House_Prices$`Random Forest` * RF_Weight + House_Prices$ANN * ANN_Weight + House_Prices$SVR * SVR_Weight + House_Prices$kNN * kNN_Weight
```


## Combined Model Results

```{r}
CombinedModel_Cor <- cor(House_Prices$`Weighted Average`, House_Prices$`True Price`)
CombinedModel_Cor
```

# Constructing the Final Prediction for the Kaggle Competition

In this chapter, we use the model that produced the best accuracy, **which is our xxxxxx model**, to predict the prices for the Ames, Iowa houses listed in the Test Data file provided in Kaggle (test.csv / rawHouseData_Test). We then create a submission file based on those predictions with which to take part in the Kaggle Competition.


## Constructing the Final Prediction


## Creating the Submission File



# Conclusion

Our objective was to predict Ames, Iowa property prices using machine learning models. As a base for our work we used an incredible Ames Housing Dataset.

Before creating our models, we did some data cleaning. Firstly, we handled all NAs in the dataset accordingly and used the Boruta Package to identify important variables in the dataset. Furthermore, we used linear and stepwise regressions to take a basic look on the significant variables effecting house prices. 
  
Based on the cleaned data, we then went on and built four different predictive models, including a Random Forest Model, an Artificial Neural Network Model, a Support Vector Machine Model, and a k-Nearest-Neighbors Model. Out of the predictions of the four models, the predictions produced with the SVR Model had the highest correlation with the actual house prices listed in our Test Dataset. The correlation between the SVR Model Predictions and actual house prices was `r round(SVR_Cor, digits = 4)`.

The other models also performed fairly well. The Random Forest Model was the second most accurate. The correlation between the predictions made with the SVR Model and the actual house prices was `r round(RF_Cor, digits = 4)` Furthermore, the correlations between the predictions made by the ANN Model and the kNN Model and the actual house prices were `r round(ANN_Cor, digits = 4)` and `r round(kNN_Cor, digits = 4)` respectively.
  
After creating the four individual models, we still thought that we could improve our predictions by obtaining a correlation-weighted average of the predictions of the individual models, since the combined model would then be trained across multiple mechanisms. The correlation between the price predictions made with our combined model and the actual house prices was `r round(CombinedModel_Cor, digits = 4)`. This correlation (accuracy) was ... **needs to be continued based on the results**

Finally, we used our best model, **xxxxxxx Model**,  to predict the prices for Ames, Iowa houses listed in a specific testing dataset provided in Kaggle and created a submission file including the predictions. We will submit that file to the Kaggle Competition that is based on the Ames Housing Dataset we have worked on.








