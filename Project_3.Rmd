---
title: "Project 3"
author: "Sean Tsai, Olli Rissanen, Kit Tsang, Ying Jie Chin, Abhinav Alluri"
date: "4/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# House Prices Prediction

## General Data Exploration

### Importing and exploring data

```{r}
rawHouseData_Train <- read.csv("train.csv", stringsAsFactors = FALSE)
rawHouseData_Test <- read.csv("test.csv", stringsAsFactors = FALSE)

str(rawHouseData_Train)
summary(rawHouseData_Train)
```

### Cleaning data
```{r}
# We need to use characters for our Boruta screening of features
CharCategory <- c(names(Filter(is.character, rawHouseData_Train)), "MSSubClass")

#Identifying the percentage of NAs in each column
library(VIM)
library(mice)
mice_plot <- aggr(rawHouseData_Train, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(rawHouseData_Train), cex.axis=.4,
                    gap=3, ylab=c("Missing data","Pattern"))

#taking care of NAs according to context
rawHouseData_Train <- rawHouseData_Train[!is.na(rawHouseData_Train$Electrical),] #dropped na;only 1 missing observation
rawHouseData_Train <- rawHouseData_Train[,!names(rawHouseData_Train)=="GarageYrBlt"] #drop GarageYrBlt column;we have many other garage variables, and imputation of year = 0 might affect the regression
rawHouseData_Train[is.na(rawHouseData_Train$MasVnrArea),] = 0 #NAs here mean that there is no Masonry veneer so area = 0

# Impute NAs by "Missing" for characters for the rest of the columns (i.e. PoolQC,MiscFeature,Alley etc) where NA = none
for(i in CharCategory){
  rawHouseData_Train[,i] = ifelse(is.na(rawHouseData_Train[,i]), "Missing", rawHouseData_Train[,i])
  rawHouseData_Test[,i] = ifelse(is.na(rawHouseData_Test[,i]), "Missing", rawHouseData_Test[,i])
}

#Imputing the NAs for Lot Frontage using predictive mean matching. 
rawHouseData_Train <- mice(rawHouseData_Train, m=5, maxit = 50, method = 'pmm', seed = 100)
rawHouseData_Train <- complete(rawHouseData_Train)
```

### Boruta Training
```{r}
# Boruta Train package to identify important features (https://www.datacamp.com/community/tutorials/feature-selection-R-boruta)
library(Boruta)
set.seed(233)
boruta_Train <- Boruta(rawHouseData_Train[names(rawHouseData_Train) != "Id" & names(rawHouseData_Train) != "SalePrice" ], rawHouseData_Train$SalePrice ,  doTrace = 2)
print(boruta_Train)
plot(boruta_Train)

# Select only those features that passed the boruta screening
cleanHouseData_Train <- rawHouseData_Train[getSelectedAttributes(boruta_Train,withTentative = F)]

# Convert characters back into factors
cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)] <- lapply(cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)], as.factor)

# Adding SalePrice back into the train data
cleanHouseData_Train$SalePrice <- rawHouseData_Train$SalePrice
```


A Look into the cleaned data
```{r}
str(cleanHouseData_Train)
summary(cleanHouseData_Train)
```

### Creating a linear regression to explore the data

Finding out the significance of the factors
- Linear Regression
- Stepwise Regression

```{r echo=T, message=FALSE, warning=FALSE}
# Basic linear regression with no further considerations
SalePrice_LinReg = lm(SalePrice ~ ., data = cleanHouseData_Train)
library(MASS)
SalePrice_StepReg = stepAIC(SalePrice_LinReg, direction = "both", trace = FALSE) 

```

Summary of the regression performance
```{r}
summary(SalePrice_StepReg)
```

## Models

### Random Forest Regression

Importing the required library
```{r}
library(caret)
library(randomForest)
```

Performing a Train/Test Split
```{r}
set.seed(300)
rfHouseData = cleanHouseData_Train
trainId = sample(1:nrow(rfHouseData),0.8*nrow(rfHouseData))
testId = setdiff(1:nrow(rfHouseData), trainId)
rfHouseDataTrain = rfHouseData[trainId,]
rfHouseDataTest = rfHouseData[testId,]
```

Contruct the model
```{r}
rfModel1 = randomForest(SalePrice ~., data = rfHouseDataTrain)
rfModel1
plot(rfModel1)
```

Predict the result on the test set
```{r}
prediction1 = predict(rfModel1, rfHouseDataTest, type = "response")
pre1 = data.frame(prediction1,rfHouseDataTest$SalePrice)
cor(prediction1,rfHouseDataTest$SalePrice)
```

- Description of results of random forest


## ANN Model

### Creating an ANN
```{r}
#data cleaning for ANN
#converting categorical data to numeric data
cleanHouseData_Train[] <- data.matrix(cleanHouseData_Train)
summary(cleanHouseData_Train)

#normalize dataset
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}
#exclude SalePrice from normalization as we want to predict SalePrice
annHouseData <- as.data.frame(lapply(cleanHouseData_Train[,!names(cleanHouseData_Train) == "SalePrice"], normalize))
summary(annHouseData)

#Include the SalePrice variable back into the dataset
annHouseData$SalePrice <- cleanHouseData_Train$SalePrice
set.seed(300)
train_id <- sample(1:nrow(annHouseData), 0.8*nrow(annHouseData))
test_id <- setdiff(1:nrow(annHouseData),train_id)
annHouseDataTrain <- annHouseData[train_id,]
annHouseDataTest <- annHouseData[test_id,]

# used only variables which had ** or *** in terms of significance level from the original lm model. When I used only variables with *** significance level, the accuracy was 0.8568247. When I used only variables with *, ** and ***, the accuracy was at 0.8646189. The best accuracy of 0.8807884 was obtained when I used variables with only ** and ***
library(nnet)
ann_model <- nnet(SalePrice ~ MSSubClass + LotArea + LandContour + OverallQual + OverallCond + MasVnrType
+ MasVnrArea + ExterQual + BsmtQual + BsmtExposure + BsmtFullBath + KitchenAbvGr + KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + GarageCars + SaleCondition,
    data=annHouseDataTrain,
    size=20, linout=TRUE, skip=TRUE, MaxNWts=10000, trace=FALSE, maxit=100)
predicted_saleprice <- predict(ann_model, newdata=annHouseDataTest[,!names(annHouseDataTest)=="SalePrice"])
cor(predicted_saleprice, annHouseDataTest$SalePrice)

#import function to plot neural network from Github found from this link: https://beckmw.wordpress.com/2013/03/04/visualizing-neural-networks-from-the-nnet-package/
require(RCurl)
 
root.url<-'https://gist.githubusercontent.com/fawda123'
raw.fun<-paste(
  root.url,
  '5086859/raw/cc1544804d5027d82b70e74b83b3941cd2184354/nnet_plot_fun.r',
  sep='/'
  )
script<-getURL(raw.fun, ssl.verifypeer = FALSE)
eval(parse(text = script))
rm('script','raw.fun')

#Plotting the neural network
par(mar=numeric(4),family='serif')
plot(ann_model,nid=F, circle.cex=2,cex=0.8, alpha.val=0)

#plotting function
library(downloader)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')


library(devtools)
library(neuralnet)
 
source_url('https://gist.github.com/fawda123/7471137/raw/c720af2cea5f312717f020a09946800d55b8f45b/nnet_plot_update.r')
  
AND <- c(rep(0,7),1)
OR <- c(0,rep(1,7))
binary.data <- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
print(net <- neuralnet(AND+OR~Var1+Var2+Var3, binary.data, hidden=0,
rep=10, err.fct="ce", linear.output=FALSE))
 
plot.nnet(net)

```

## Support Vector Machine
```{r}
# Randomize data
svm_HouseData = cleanHouseData_Train

# Divide into training and test data
svm_train = svm_HouseData[trainId,]
svm_test = svm_HouseData[testId,]

# Training a SVM model (kernal "vanilladot" produces the lowerst rmse after trials)
library(kernlab)
model_svm1 <- ksvm(SalePrice ~ ., data = svm_train,
                          kernel = "vanilladot")

# Basic information about the model
model_svm1

# Prediction on testing data
prediction_svm1 <- predict(model_svm1, svm_test)

# SVM model 2 (build the model by using the significant factors shown in the regression model) 
model_svm2 <- ksvm(SalePrice ~ MSZoning+ LotArea+ LotShape+ LandContour+ LandSlope+ Neighborhood+ BldgType+ OverallQual+ OverallCond+ Exterior1st+ Exterior2nd+ ExterQual+ BsmtQual+ BsmtExposure+ BsmtFinType1+ BsmtFullBath+ FullBath+ KitchenQual+ TotRmsAbvGrd+ Functional, data = svm_train, kernel = "vanilladot")

# Basic information about the model
model_svm2
# Prediction on testing data
prediction_svm2 <- predict(model_svm2, svm_test)

# SVM model 3 (using the significant factors shown in the stepwise regression model)
model_svm3 <- ksvm(SalePrice ~ MSZoning + LotArea + LotShape + LandContour + 
    LandSlope + Neighborhood + Condition1 + BldgType + HouseStyle + 
    OverallQual + OverallCond + YearBuilt + Exterior1st + MasVnrArea + 
    ExterQual + BsmtQual + BsmtExposure + BsmtFinType1 + X2ndFlrSF + 
    GrLivArea + BsmtFullBath + FullBath + BedroomAbvGr + KitchenAbvGr + 
    KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + GarageCars + 
    GarageQual + GarageCond + WoodDeckSF + SaleCondition, data = svm_train, kernel = "vanilladot")

# Basic information about the model
model_svm3
# Prediction on testing data
prediction_svm3 <- predict(model_svm3, svm_test)

```

## KNN
```{r}
cleanHouseData_random <- cleanHouseData_Train[sample(nrow(cleanHouseData_Train)),]
cleanHouseData_rand <- as.data.frame(model.matrix(~ . -1, data=cleanHouseData_random))
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}
cleanHouseData_norm <- as.data.frame(lapply(cleanHouseData_rand, normalize))
str(cleanHouseData_norm)
data("cleanHouseData_norm")
data <- cleanHouseData_norm
str(data)
set.seed(1234)
ind <- sample(2, nrow(data), replace = T, prob = c(.7,.3))
training <- data[ind == 1,]
test <- data[ind == 2,]
trControl <- trainControl(method = 'repeatedcv',
                          number = 10,
                          repeats = 3)
set.seed(333)
fit <- train(SalePrice ~.,
             data = training,
             tuneGrid = expand.grid(k=1:70),
             method = 'knn',
             trControl = trControl,
             preProc = c('center', 'scale'))
fit
```


