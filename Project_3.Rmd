---
title: "Project 3"
author: "Sean Tsai, Olli Rissanen, Kit Tsang, Ying Jie Chin, Abhinav Alluri"
date: "4/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# House Prices Prediction

## Step 1: Importing and Cleaning Data

### Importing and exploring data

```{r}
rawHouseData_Train <- read.csv("train.csv", stringsAsFactors = FALSE)
rawHouseData_Test <- read.csv("test.csv", stringsAsFactors = FALSE)

str(rawHouseData_Train)
summary(rawHouseData_Train)
```

Since the test set in this case only has the predictor variable

### Cleaning data with Boruta Screening

```{r}
# We need to use characters for our Boruta screening of features
CharCategory <- c(names(Filter(is.character, rawHouseData_Train)), "MSSubClass")

#Identifying the percentage of NAs in each column
library(VIM)
library(mice)
mice_plot <- aggr(rawHouseData_Train, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(rawHouseData_Train), cex.axis=.4,
                    gap=3, ylab=c("Missing data","Pattern"))

#taking care of NAs for TRAIN set according to context
rawHouseData_Train <- rawHouseData_Train[!is.na(rawHouseData_Train$Electrical),] #dropped na;only 1 missing observation
rawHouseData_Train <- rawHouseData_Train[,!names(rawHouseData_Train)=="GarageYrBlt"] #drop GarageYrBlt column;we have many other garage variables, and imputation of year = 0 might affect the regression
rawHouseData_Train[is.na(rawHouseData_Train$MasVnrArea),] = 0 #NAs here mean that there is no Masonry veneer so area = 0

# Impute NAs by "Missing" for characters for the rest of the columns (i.e. PoolQC,MiscFeature,Alley etc) where NA = none
for(i in CharCategory){
  rawHouseData_Train[,i] = ifelse(is.na(rawHouseData_Train[,i]), "Missing", rawHouseData_Train[,i])
  rawHouseData_Test[,i] = ifelse(is.na(rawHouseData_Test[,i]), "Missing", rawHouseData_Test[,i])
}


#Imputing the NAs for Lot Frontage using predictive mean matching. 
rawHouseData_Train <- mice(rawHouseData_Train, m=5, maxit = 50, method = 'pmm', seed = 100)
rawHouseData_Train <- complete(rawHouseData_Train)

# Boruta Train package to identify important features (https://www.datacamp.com/community/tutorials/feature-selection-R-boruta)
library(Boruta)
set.seed(233)
boruta_Train <- Boruta(rawHouseData_Train[names(rawHouseData_Train) != "Id" & names(rawHouseData_Train) != "SalePrice" ], rawHouseData_Train$SalePrice ,  doTrace = 2)
print(boruta_Train)
plot(boruta_Train)

# Select only those features that passed the boruta screening
cleanHouseData_Train <- rawHouseData_Train[getSelectedAttributes(boruta_Train, withTentative = F)]

# Convert characters back into factors
cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)] <- lapply(cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)], as.factor)

# Adding SalePrice back into the train data
cleanHouseData_Train$SalePrice <- rawHouseData_Train$SalePrice

```

**COMMENTS:**

**For print(boruta_Train), text "9 tentative attributes left" is printed. These attributes are later removed, but do we actually and certainly know that these attributes are not important as there are also attributes that are confirmed as important and unimportant?**

**Why are we converting stuff BACK into factors in the end? Seemed that there were no factors in the data originally based on the first initial str when the data was read. Or are we now creating factors for the first time as it's necessary?** 

**Added SalePrice to the cleaned train data. Does Id need to be added as well? Or should it be (kept) removed as it does not explain SalePrice for sure being basically a row number?**

**As this is a competition and for the competition, we should probably not randomize the data. Thought based on the sample_submission file. Also, the train and test data were given.**

### A look into the cleaned data

```{r}
str(cleanHouseData_Train)
summary(cleanHouseData_Train)
```

## Step 2: Linear Regression

### Creating a linear regression

```{r}
# Basic linear regression with no further considerations
SalePrice_LinReg <- lm(SalePrice ~ ., data = cleanHouseData_Train)
library(MASS)
step_LinReg <- stepAIC(SalePrice_LinReg, direction = "both", 
                      trace = FALSE)
```

### Summary of the regression performance

```{r}
summary(SalePrice_LinReg)
summary(step_LinReg)
```

**Based on the first look there are quite a few insignificant attributes. Is the Boruta wiser or should we do further cleaning based on the regression results?**


## Artificial Neural Networks

### Creating an ANN

```{r}
#data cleaning for ANN

#converting categorical data to numeric data
cleanHouseData_Train[] <- data.matrix(cleanHouseData_Train)
summary(cleanHouseData_Train)

#normalize dataset
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}

#exclude SalePrice from normalization as we want to predict SalePrice
annHouseData <- as.data.frame(lapply(cleanHouseData_Train[,!names(cleanHouseData_Train) == "SalePrice"], normalize))
summary(annHouseData)

#Include the SalePrice variable back into the dataset
annHouseData$SalePrice <- cleanHouseData_Train$SalePrice

set.seed(300)
train_id <- sample(1:nrow(annHouseData), 0.8*nrow(annHouseData))
test_id <- setdiff(1:nrow(annHouseData),train_id)
annHouseDataTrain <- annHouseData[train_id,]
annHouseDataTest <- annHouseData[test_id,]

# used only variables which had ** or *** in terms of significance level from the original lm model. When I used ony variables with *** significance level, the accuracy was 0.8568247. When I used only variables with *, ** and ***, the accuracy was at 0.8646189. The best accuracy of 0.8807884 was obtained when I used variables with only ** and ***

library(nnet)
ann_model <- nnet(SalePrice ~ MSSubClass + LotArea + LandContour + OverallQual + OverallCond + MasVnrType
+ MasVnrArea + ExterQual + BsmtQual + BsmtExposure + BsmtFullBath + KitchenAbvGr + KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + GarageCars + SaleCondition,
    data=annHouseDataTrain,
    size=10, linout=TRUE, skip=TRUE, MaxNWts=10000, trace=FALSE, maxit=100)

predicted_saleprice <- predict(ann_model, newdata=annHouseDataTest[,!names(annHouseDataTest)=="SalePrice"])
cor(predicted_saleprice, annHouseDataTest$SalePrice)

#import function to plot neural network from Github found from this link: https://beckmw.wordpress.com/2013/03/04/visualizing-neural-networks-from-the-nnet-package/

require(RCurl)
root.url<-'https://gist.githubusercontent.com/fawda123'
raw.fun<-paste(
  root.url,
  '5086859/raw/cc1544804d5027d82b70e74b83b3941cd2184354/nnet_plot_fun.r',
  sep='/'
  )
script<-getURL(raw.fun, ssl.verifypeer = FALSE)
eval(parse(text = script))
rm('script','raw.fun')

#Plotting the neural network
par(mar=numeric(4),family='serif')
plot(ann_model,nid=F, circle.cex=2,cex=0.8)

```