---
title: "Project 3"
author: "Sean Tsai, Olli Rissanen, Kit Tsang, Ying Jie Chin, Abhinav Alluri"
date: "4/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# House Prices Prediction

## General Data Exploration

### Importing and exploring data

```{r}
rawHouseData_Train <- read.csv("train.csv", stringsAsFactors = FALSE)
rawHouseData_Test <- read.csv("test.csv", stringsAsFactors = FALSE)

str(rawHouseData_Train)
summary(rawHouseData_Train)
```

### Cleaning data
```{r}
# We need to use characters for our Boruta screening of features
CharCategory <- c(names(Filter(is.character, rawHouseData_Train)), "MSSubClass")

#Identifying the percentage of NAs in each column
library(VIM)
library(mice)
mice_plot <- aggr(rawHouseData_Train, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(rawHouseData_Train), cex.axis=.4,
                    gap=3, ylab=c("Missing data","Pattern"))

#taking care of NAs according to context
rawHouseData_Train <- rawHouseData_Train[!is.na(rawHouseData_Train$Electrical),] #dropped na;only 1 missing observation
rawHouseData_Train <- rawHouseData_Train[,!names(rawHouseData_Train)=="GarageYrBlt"] #drop GarageYrBlt column;we have many other garage variables, and imputation of year = 0 might affect the regression
rawHouseData_Train[is.na(rawHouseData_Train$MasVnrArea),] = 0 #NAs here mean that there is no Masonry veneer so area = 0

# Impute NAs by "Missing" for characters for the rest of the columns (i.e. PoolQC,MiscFeature,Alley etc) where NA = none
for(i in CharCategory){
  rawHouseData_Train[,i] = ifelse(is.na(rawHouseData_Train[,i]), "Missing", rawHouseData_Train[,i])
  rawHouseData_Test[,i] = ifelse(is.na(rawHouseData_Test[,i]), "Missing", rawHouseData_Test[,i])
}

#Imputing the NAs for Lot Frontage using predictive mean matching. 
rawHouseData_Train <- mice(rawHouseData_Train, m=5, maxit = 50, method = 'pmm', seed = 100)
rawHouseData_Train <- complete(rawHouseData_Train)
```

### Boruta Training
```{r}
# Boruta Train package to identify important features (https://www.datacamp.com/community/tutorials/feature-selection-R-boruta)
library(Boruta)
set.seed(233)
boruta_Train <- Boruta(rawHouseData_Train[names(rawHouseData_Train) != "Id" & names(rawHouseData_Train) != "SalePrice" ], rawHouseData_Train$SalePrice ,  doTrace = 2)
print(boruta_Train)
plot(boruta_Train)

# Select only those features that passed the boruta screening
cleanHouseData_Train <- rawHouseData_Train[getSelectedAttributes(boruta_Train,withTentative = F)]

# Convert characters back into factors
cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)] <- lapply(cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)], as.factor)

# Adding SalePrice back into the train data
cleanHouseData_Train$SalePrice <- rawHouseData_Train$SalePrice
```


A Look into the cleaned data
```{r}
str(cleanHouseData_Train)
summary(cleanHouseData_Train)
```

### Creating a linear regression to explore the data

Finding out the significance of the factors
- Linear Regression
- Stepwise Regression

```{r}
# Basic linear regression with no further considerations
SalePrice_LinReg = lm(SalePrice ~ ., data = cleanHouseData_Train)
library(MASS)
SalePrice_StepReg = stepAIC(SalePrice_LinReg, direction = "both", trace = FALSE)
```

Summary of the regression performance
```{r}
summary(SalePrice_StepReg)
```

## Models

### Random Forest Regression

Importing the required library
```{r}
library(caret)
library(randomForest)
```

Performing a Train/Test Split   
(The Train/Test Ids are also used for the remaining models to make sure that the training and testing sets are the same)
```{r}
set.seed(300)
rfHouseData = cleanHouseData_Train
trainId = sample(1:nrow(rfHouseData),0.8*nrow(rfHouseData))
testId = setdiff(1:nrow(rfHouseData), trainId)
rfHouseDataTrain = rfHouseData[trainId,]
rfHouseDataTest = rfHouseData[testId,]
```

Contruct the model
```{r}
rfModel1 = randomForest(SalePrice ~., data = rfHouseDataTrain)
rfModel1
```

A look into the process of the model
```{r}
plot(rfModel1)
```

Predict the result on the test set
```{r}
prediction1 = predict(rfModel1, rfHouseDataTest, type = "response")
pre1 = data.frame(prediction1,rfHouseDataTest$SalePrice)
```

The correlation between the test set and the model prediction is the following:  
```{r}
cor(prediction1,rfHouseDataTest$SalePrice)

```

From the random forest model, the strong correlation of `r cor(prediction1,rfHouseDataTest$SalePrice)` here implies precise prediction that the predicted data goes in the same trend with the true result. We will take this as our final result for the random forest model.

```{r}
rfFinalPrediction = prediction1
```



We will then go on and evaluate more other models and compare the results.


### ANN

Import required libraries
```{r}
library(nnet)
library(RCurl)
```

Convert the numeric data into categorical data for ANN usage
```{r}
#converting categorical data to numeric data
cleanHouseData_Train[] <- data.matrix(cleanHouseData_Train)
summary(cleanHouseData_Train)
```

Normalizing the data
```{r}
#normalize dataset
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}

#exclude SalePrice from normalization as we want to predict SalePrice
annHouseData <- as.data.frame(lapply(cleanHouseData_Train[,!names(cleanHouseData_Train) == "SalePrice"], normalize))
summary(annHouseData)

#Include the SalePrice variable back into the dataset
annHouseData$SalePrice <- cleanHouseData_Train$SalePrice
```

Train/Test Split
```{r}
annHouseDataTrain <- annHouseData[trainId,]
annHouseDataTest <- annHouseData[testId,]
```

Model Building.  
```{r}
library(nnet)
ann_model <- nnet(SalePrice ~ MSSubClass + LotArea + LandContour + OverallQual + OverallCond + MasVnrType
+ MasVnrArea + ExterQual + BsmtQual + BsmtExposure + BsmtFullBath + KitchenAbvGr + KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + GarageCars + SaleCondition,
    data=annHouseDataTrain,
    size=10, linout=TRUE, skip=TRUE, MaxNWts=10000, trace=FALSE, maxit=100)

```

Model Results.
  
We used only variables which had ** or *** in terms of significance level from the original lm model. When I used only variables with *** significance level, the accuracy was 0.8568247. When I used only variables with \*, ** and ***, the accuracy was at 0.8646189. The best accuracy of 0.8807884 was obtained when I used variables with only ** and ***.  
```{r}
predicted_saleprice <- predict(ann_model, newdata=annHouseDataTest[,!names(annHouseDataTest)=="SalePrice"])
cor(predicted_saleprice, annHouseDataTest$SalePrice)
```

We will thus take the model yielding the highest correlation as our final model for ANN.
```{r}
annFinalPrediction = predicted_saleprice
```


### Support Vector Machine
Importing the required library
```{r}
library(kernlab)
```

Import the data
```{r}
svmHouseData = cleanHouseData_Train
```

Performing a Train/Test Split
```{r}
svm_train = rfHouseData[trainId,]
svm_test = rfHouseData[testId,]
```

Contruct the model  
6 different Models are used. The first model includes all the variables of the dataset. The second model includes all the significant factors of the linear regression. The third model includes all the significant factors of the stepwise regression.  
The models 4-6 are using the same input as the first three models but using different kernels (rbf).
```{r}
model_svm1 <- ksvm(SalePrice ~ ., data = svm_train,
                          kernel = "vanilladot")
model_svm2 <- ksvm(SalePrice ~ MSZoning+ LotArea+ LotShape+ LandContour+ LandSlope+ Neighborhood+ BldgType+ OverallQual+ OverallCond+ Exterior1st+ Exterior2nd+ ExterQual+ BsmtQual+ BsmtExposure+ BsmtFinType1+ BsmtFullBath+ FullBath+ KitchenQual+ TotRmsAbvGrd+ Functional, data = svm_train, kernel = "vanilladot")
model_svm3 <- ksvm(SalePrice ~ MSZoning + LotArea + LotShape + LandContour + 
    LandSlope + Neighborhood + Condition1 + BldgType + HouseStyle + 
    OverallQual + OverallCond + YearBuilt + Exterior1st + MasVnrArea + 
    ExterQual + BsmtQual + BsmtExposure + BsmtFinType1 + X2ndFlrSF + 
    GrLivArea + BsmtFullBath + FullBath + BedroomAbvGr + KitchenAbvGr + 
    KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + GarageCars + 
    GarageQual + GarageCond + WoodDeckSF + SaleCondition, data = svm_train, kernel = "vanilladot")


model_svm4 <- ksvm(SalePrice ~ ., data = svm_train,
                          kernel = "rbfdot")
model_svm5 <- ksvm(SalePrice ~ MSZoning+ LotArea+ LotShape+ LandContour+ LandSlope+ Neighborhood+ BldgType+ OverallQual+ OverallCond+ Exterior1st+ Exterior2nd+ ExterQual+ BsmtQual+ BsmtExposure+ BsmtFinType1+ BsmtFullBath+ FullBath+ KitchenQual+ TotRmsAbvGrd+ Functional, data = svm_train, kernel = "rbfdot")
model_svm6 <- ksvm(SalePrice ~ MSZoning + LotArea + LotShape + LandContour + 
    LandSlope + Neighborhood + Condition1 + BldgType + HouseStyle + 
    OverallQual + OverallCond + YearBuilt + Exterior1st + MasVnrArea + 
    ExterQual + BsmtQual + BsmtExposure + BsmtFinType1 + X2ndFlrSF + 
    GrLivArea + BsmtFullBath + FullBath + BedroomAbvGr + KitchenAbvGr + 
    KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + GarageCars + 
    GarageQual + GarageCond + WoodDeckSF + SaleCondition, data = svm_train, kernel = "rbfdot")


```

Predict the result on the test set  
```{r}
prediction_svm1 <- predict(model_svm1, svm_test)
prediction_svm2 <- predict(model_svm2, svm_test)
prediction_svm3 <- predict(model_svm3, svm_test)
cor(prediction_svm1, svm_test$SalePrice)
cor(prediction_svm2, svm_test$SalePrice)
cor(prediction_svm3, svm_test$SalePrice)


prediction_svm4 <- predict(model_svm4, svm_test)
prediction_svm5 <- predict(model_svm5, svm_test)
prediction_svm6 <- predict(model_svm6, svm_test)
cor(prediction_svm4, svm_test$SalePrice)
cor(prediction_svm5, svm_test$SalePrice)
cor(prediction_svm6, svm_test$SalePrice)
```
6 SVM models were constructed and we would evaluate the accuracy based on correlation between the predictions of the models and the sales price of the test data. Model 1,2, and 3 had an accuracy of 0.863, 0.896, and 0.896 respectively. Meanwhile, model 4,5, and 6 had an accuracy of 0.943, 0.914, and 0.941 respectively.

The model using all the factors from the dataset together with the rbf kernel trick is giving us the best result. We will take that as our final prediction.
```{r}
svmFinalPrediction = prediction_svm4
```


### KNN

Use model matrix to perform dummy encoding.
```{r}
cleanHouseData_rand <- as.data.frame(model.matrix(~ . -1, data= cleanHouseData_Train))
```

Normalize the data for KNN usage.
```{r}
cleanHouseData_norm <- as.data.frame(lapply(cleanHouseData_rand[,!names(cleanHouseData_rand) == "SalePrice"], normalize))
cleanHouseData_norm$SalePrice <- cleanHouseData_rand$SalePrice

str(cleanHouseData_norm)
```

Train/Test Split
```{r}
knnData = cleanHouseData_norm

knnDataTrain = knnData[trainId,]
knnDataTest = knnData[testId,]
```

Train control for KNN, we used a repeated 10-fold cross-validation here to find the optimized number of K in the model.
```{r}
trControl <- trainControl(method = 'repeatedcv',number = 10, repeats = 3)
```

Build the KNN model.
```{r}
set.seed(333)
knn_model <- train(SalePrice ~., data = knnDataTrain,tuneGrid = expand.grid(k=1:70), method = 'knn', trControl = trControl,preProc = c('center', 'scale'))
```

Visualizing the process of the cross validation in finding the best K by minimizing the RMSE. The variable importance within the model is also listed below.
```{r}
plot(knn_model)

varImp(knn_model)
```

Prediction on the testing set.
```{r}
knnPrediction <- predict(knn_model, newdata = knnDataTest)
```

Model Evaluation
```{r}
plot(knnPrediction ~ knnDataTest$SalePrice)
cor(knnPrediction,knnDataTest$SalePrice)
```

The correlation of the cross-validated KNN model turns out to be `r cor(knnPrediction,knnDataTest$SalePrice)`, which is really high. We will take this as our final result.

```{r}
knnFinalPrediction = knnPrediction
```


### Combining all models

Build the dataframe containing all the prediction results.
```{r}
predResults = data.frame(rfFinalPrediction,annFinalPrediction,svmFinalPrediction,knnFinalPrediction, rfHouseDataTest$SalePrice)
colnames(predResults) = c("Random Forest", "ANN", "SVM", "KNN", "True Price")
```

Use the strength of correlation to give a weighted average of each model's final output.
```{r}
rfCor = cor(predResults$`Random Forest`,predResults$`True Price`)
annCor = cor(predResults$ANN,predResults$`True Price`)
svmCor = cor(predResults$SVM,predResults$`True Price`)
knnCor = cor(predResults$KNN,predResults$`True Price`)
sumOfCor = rfCor+annCor+svmCor+knnCor

rfWeight = rfCor/sumOfCor
annWeight = annCor/sumOfCor
svmWeight = svmCor/sumOfCor
knnWeight = knnCor/sumOfCor

#Taking weighted average of predictions
predResults[,"Weighted Average"] = predResults$`Random Forest`*rfWeight+ predResults$ANN*annWeight+ predResults$SVM*svmWeight+ predResults$KNN*knnWeight
```

The correlation for the combined model.
```{r}
cor(predResults$`Weighted Average`, predResults$`True Price`)
```

## Conclusion

Our original objective of our group was to predict the housing prices of IOWA based on machine learning models. Firstly, we used the Boruta package to identify important variables in the dataset. Also, we used linear regression to have a quick glance on significant variables. 
  
Based on that, we then went on and built four different models including Random Forest, Artificial Neural Network, Support Vector Regression, and K-nearest-neighbors.  
  
Out of the four models, the model with the highest correlation of `r cor(predResults$`Random Forest`,predResults$`True Price`)` with the testing set was random forest. This is followed by SVM's `r cor(predResults$SVM,predResults$`True Price`)`, ANN's `r cor(predResults$ANN,predResults$`True Price`)`and KNN's `r cor(predResults$KNN,predResults$`True Price`)` in correlation.   
  
We believe that our rigor of the models can be improved by obtaining a weighted-average of each of the individual models, since it was trained across multiple mechanisms. The correlation of our combined model was `r cor(predResults$`Weighted Average`, predResults$`True Price`)`.




