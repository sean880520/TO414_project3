---
title: "Project 3"
author: "Sean Tsai, Olli Rissanen, Kit Tsang, Ying Jie Chin, Abhinav Alluri"
date: "4/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# House Prices Prediction

## Step 1: Importing and Cleaning Data

### Importing and exploring data

```{r}
rawHouseData_Train <- read.csv("train.csv", stringsAsFactors = FALSE)
rawHouseData_Test <- read.csv("test.csv", stringsAsFactors = FALSE)

str(rawHouseData_Train)
summary(rawHouseData_Train)
```

### Cleaning data with Boruta Screening

```{r}
# We need to use characters for our Boruta screening of features
CharCategory <- c(names(Filter(is.character, rawHouseData_Train)), "MSSubClass")

#Identifying the percentage of NAs in each column
library(VIM)
library(mice)
mice_plot <- aggr(rawHouseData_Train, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(rawHouseData_Train), cex.axis=.4,
                    gap=3, ylab=c("Missing data","Pattern"))

#taking care of NAs according to context
rawHouseData_Train <- rawHouseData_Train[!is.na(rawHouseData_Train$Electrical),] #dropped na;only 1 missing observation
rawHouseData_Train <- rawHouseData_Train[,!names(rawHouseData_Train)=="GarageYrBlt"] #drop GarageYrBlt column;we have many other garage variables, and imputation of year = 0 might affect the regression
rawHouseData_Train[is.na(rawHouseData_Train$MasVnrArea),] = 0 #NAs here mean that there is no Masonry veneer so area = 0

# Impute NAs by "Missing" for characters for the rest of the columns (i.e. PoolQC,MiscFeature,Alley etc) where NA = none
for(i in CharCategory){
  rawHouseData_Train[,i] = ifelse(is.na(rawHouseData_Train[,i]), "Missing", rawHouseData_Train[,i])
  rawHouseData_Test[,i] = ifelse(is.na(rawHouseData_Test[,i]), "Missing", rawHouseData_Test[,i])
}


#Imputing the NAs for Lot Frontage using predictive mean matching. 
rawHouseData_Train <- mice(rawHouseData_Train, m=5, maxit = 50, method = 'pmm', seed = 100)
rawHouseData_Train <- complete(rawHouseData_Train)

# Boruta Train package to identify important features (https://www.datacamp.com/community/tutorials/feature-selection-R-boruta)
library(Boruta)
set.seed(233)
boruta_Train <- Boruta(rawHouseData_Train[names(rawHouseData_Train) != "Id" & names(rawHouseData_Train) != "SalePrice" ], rawHouseData_Train$SalePrice ,  doTrace = 2)
print(boruta_Train)
plot(boruta_Train)

# Select only those features that passed the boruta screening
cleanHouseData_Train <- rawHouseData_Train[getSelectedAttributes(boruta_Train, withTentative = F)]

# Convert characters back into factors
cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)] <- lapply(cleanHouseData_Train[sapply(cleanHouseData_Train,is.character)], as.factor)

# Adding SalePrice back into the train data
cleanHouseData_Train$SalePrice <- rawHouseData_Train$SalePrice
```

**COMMENTS:**

**For print(boruta_Train), text "9 tentative attributes left" is printed. These attributes are later removed, but do we actually and certainly know that these attributes are not important as there are also attributes that are confirmed as important and unimportant?**

**Why are we converting stuff BACK into factors in the end? Seemed that there were no factors in the data originally based on the first initial str when the data was read. Or are we now creating factors for the first time as it's necessary?** 

**Added SalePrice to the cleaned train data. Does Id need to be added as well? Or should it be (kept) removed as it does not explain SalePrice for sure being basically a row number?**

**As this is a competition and for the competition, we should probably not randomize the data. Thought based on the sample_submission file. Also, the train and test data were given.**

### A look into the cleaned data

```{r}
str(cleanHouseData_Train)
summary(cleanHouseData_Train)
```

## Step 2: Linear Regression

### Creating a linear regression

```{r}
# Basic linear regression with no further considerations
SalePrice_LinReg <- lm(SalePrice ~ ., data = cleanHouseData_Train)
```


### Summary of the regression performance

```{r}
summary(SalePrice_LinReg)
```

**Based on the first look there are quite a few insignificant attributes. Is the Boruta wiser or should we do further cleaning based on the regression results?**

## SVM Model

**Based on the linear regression model, significant factors include MSZoning, LotArea, LotShape, LandContour, LandSlope, Neighborhood, BldgType, OverallQual, OverallCond, Exterior1st, Exterior2nd, ExterQual, BsmtQual, BsmtExposure, BsmtFinType1, BsmtFullBath, FullBath, KitchenQual, TotRmsAbvGrd, Functional**

```{r}
# Randomize data
set.seed(300)
svm <- cleanHouseData_Train[sample(nrow(cleanHouseData_Train)),]

# Divide into training and test data
svm_train <- svm[1:1167, ]
svm_test  <- svm[1168:1459, ]

# Training a SVM model (kernal "vanilladot" produces the lowerst rmse after trials)
library(kernlab)
model_svm1 <- ksvm(SalePrice ~ ., data = svm_train,
                          kernel = "vanilladot")

# Basic information about the model
model_svm1

# Prediction on testing data
prediction_svm1 <- predict(model_svm1, svm_test)

# RMSE Calculation
se = 0
for (i in 1:nrow(prediction_svm1)) {
  se = se + (prediction_svm1[i,1]-svm_test$SalePrice[i])^2
}
rmse_1 = (se/nrow(prediction_svm1))^0.5
rmse_1

# SVM model 2 (build the model by using the significant factors shown in the regression model) 

model_svm2 <- ksvm(SalePrice ~ MSZoning+ LotArea+ LotShape+ LandContour+ LandSlope+ Neighborhood+ BldgType+ OverallQual+ OverallCond+ Exterior1st+ Exterior2nd+ ExterQual+ BsmtQual+ BsmtExposure+ BsmtFinType1+ BsmtFullBath+ FullBath+ KitchenQual+ TotRmsAbvGrd+ Functional, data = svm_train, kernel = "vanilladot")

# Basic information about the model
model_svm2

# Prediction on testing data
prediction_svm2 <- predict(model_svm2, svm_test)

# RMSE Calculation
se = 0
for (i in 1:nrow(prediction_svm2)) {
  se = se + (prediction_svm2[i,1]-svm_test$SalePrice[i])^2 
}
rmse_2 = (se/nrow(prediction_svm2))^0.5
rmse_2

# SVM model 3 (using the significant factors shown in the stepwise regression model)
model_svm3 <- ksvm(SalePrice ~ MSZoning + LotArea + LotShape + LandContour + 
    LandSlope + Neighborhood + Condition1 + BldgType + HouseStyle + 
    OverallQual + OverallCond + YearBuilt + Exterior1st + MasVnrArea + 
    ExterQual + BsmtQual + BsmtExposure + BsmtFinType1 + X2ndFlrSF + 
    GrLivArea + BsmtFullBath + FullBath + BedroomAbvGr + KitchenAbvGr + 
    KitchenQual + TotRmsAbvGrd + Functional + Fireplaces + GarageCars + 
    GarageQual + GarageCond + WoodDeckSF + SaleCondition, data = svm_train, kernel = "vanilladot")

# Basic information about the model
model_svm3

# Prediction on testing data
prediction_svm3 <- predict(model_svm3, svm_test)

# RMSE Calculation
se = 0
for (i in 1:nrow(prediction_svm3)) {
  se = se + (prediction_svm3[i,1]-svm_test$SalePrice[i])^2 
}
rmse_3 = (se/nrow(prediction_svm3))^0.5
rmse_3
```

